{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2022-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbpyth97/Exercise/blob/master/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "30th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi",
        "outputId": "d687dc12-edc8-4be7-bbaa-0fc3d290e53e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "outputId": "fc071109-fd94-4ccc-960d-4a26c4cfd43c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 31 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 48.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(20, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "\n",
        "query = lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode('append') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(600)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext,1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "  line = lines.window(1,24)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1]),x.split(' ')[-3]))\\\n",
        "  .reduceByKey(lambda a,b: max(a,b))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(75)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj",
        "outputId": "d52ef818-f070-4913-8d44-4ba67b149123",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-19 21:52:02\n",
            "-------------------------------------------\n",
            "('10161 2020-01-01T22:23:11', '43.33')\n",
            "('14857 2020-01-01T22:24:57', '92.8')\n",
            "('14858 2020-01-01T22:25:04', '120.68')\n",
            "('10161 2020-01-01T22:28:03', '47.67')\n",
            "('19563 2020-01-01T22:29:46', '48.75')\n",
            "('10161 2020-01-01T22:30:29', '46.97')\n",
            "('13691 2020-01-01T22:31:33', '37.87')\n",
            "('19563 2020-01-01T22:32:12', '44.6')\n",
            "('19563 2020-01-01T22:34:38', '42.35')\n",
            "('14858 2020-01-01T22:35:01', '125.72')\n",
            "...\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-19 21:52:26\n",
            "-------------------------------------------\n",
            "('14857 2020-01-02T22:22:56', '163.18')\n",
            "('2332 2020-01-02T22:23:14', '51.37')\n",
            "('19563 2020-01-02T22:25:05', '22.65')\n",
            "('14857 2020-01-02T22:25:27', '148.75')\n",
            "('14858 2020-01-02T22:26:00', '175.87')\n",
            "('10161 2020-01-02T22:26:37', '105.63')\n",
            "('19563 2020-01-02T22:27:31', '18.33')\n",
            "('14858 2020-01-02T22:28:28', '182.12')\n",
            "('10161 2020-01-02T22:29:03', '106.57')\n",
            "('14857 2020-01-02T22:30:23', '150.85')\n",
            "...\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-19 21:52:50\n",
            "-------------------------------------------\n",
            "('2332 2020-01-03T22:23:36', '158.97')\n",
            "('13691 2020-01-03T22:24:53', '15.83')\n",
            "('13691 2020-01-03T22:27:19', '15.3')\n",
            "('14858 2020-01-03T22:29:19', '275.25')\n",
            "('19563 2020-01-03T22:30:27', '22.45')\n",
            "('10161 2020-01-03T22:30:35', '130.73')\n",
            "('19563 2020-01-03T22:32:55', '24.67')\n",
            "('14858 2020-01-03T22:34:17', '262.9')\n",
            "('13691 2020-01-03T22:34:39', '16.97')\n",
            "('19563 2020-01-03T22:35:21', '23.48')\n",
            "...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average air quality, for each location. Updated hourly;"
      ],
      "metadata": {
        "id": "niYfjlYZ034R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1]))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rZnFTf83TtQA",
        "outputId": "c886092a-f0a9-4e76-bb64-bdc084b472a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-19 22:41:25\n",
            "-------------------------------------------\n",
            "('13691 00', 35.11125)\n",
            "('14857 00', 231.14857142857142)\n",
            "('10161 00', 118.24857142857142)\n",
            "('14858 00', 304.90999999999997)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-19 22:41:26\n",
            "-------------------------------------------\n",
            "('14857 00', 155.864375)\n",
            "('13691 00', 25.865882352941178)\n",
            "('14858 01', 228.805)\n",
            "('14858 00', 245.211875)\n",
            "('10161 00', 117.74941176470587)\n",
            "('2332 00', 549.0566666666667)\n",
            "('14857 01', 126.57000000000001)\n",
            "('2332 01', 445.40875)\n",
            "('10161 01', 114.0325)\n",
            "('13691 01', 38.93571428571429)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-19 22:41:27\n",
            "-------------------------------------------\n",
            "('14858 01', 204.768)\n",
            "('13691 02', 41.47125)\n",
            "('13691 01', 17.766470588235293)\n",
            "('10161 01', 95.7775)\n",
            "('2332 01', 395.1673333333334)\n",
            "('14857 01', 102.568)\n",
            "('10161 02', 74.6575)\n",
            "('2332 02', 377.38375)\n",
            "('14857 02', 78.38714285714286)\n",
            "('14858 02', 166.01666666666665)\n",
            "...\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-19 22:41:28\n",
            "-------------------------------------------\n",
            "('13691 02', 11.905624999999999)\n",
            "('13691 03', 11.361250000000002)\n",
            "('14858 02', 153.53466666666665)\n",
            "('10161 02', 75.5464705882353)\n",
            "('14857 02', 70.39538461538463)\n",
            "('2332 02', 273.03)\n",
            "('14857 03', 61.294999999999995)\n",
            "('10161 03', 78.685)\n",
            "('14858 03', 131.36166666666668)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-19 22:41:29\n",
            "-------------------------------------------\n",
            "('13691 03', 10.903333333333334)\n",
            "('2332 04', 203.71571428571426)\n",
            "('14857 04', 47.722857142857144)\n",
            "('14858 03', 134.22714285714284)\n",
            "('14857 03', 60.48307692307692)\n",
            "('10161 03', 70.55875)\n",
            "('2332 03', 319.235)\n",
            "('14858 04', 122.2625)\n",
            "('13691 04', 10.167142857142858)\n",
            "('10161 04', 63.31428571428571)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}