{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2022-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbpyth97/Exercise/blob/master/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "30th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi",
        "outputId": "8d9d29a2-8227-44cc-aa21-400a40a06ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "outputId": "fe4d4375-4559-4552-eb05-d480a030dd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 60.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form",
        "outputId": "ce9de116-3c89-4f45-c4f0-d3af424c2c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting /tmp/kraft-combined-logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1",
        "outputId": "d53c7640-e16d-448e-cb0c-e9fc22f37b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(5, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "#top5 = lines.groupBy(window(lines.timestamp, '24 seconds', '24 seconds'), 'location') \\\n",
        "#       .count()\n",
        "\n",
        "query = lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode('append') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "query.awaitTermination(30)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5fc10bf840c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumpBatchDF\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"Stop this streaming query.\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "\n",
        "sl = split(lines.value, ' ')\n",
        "\n",
        "results = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string')) \\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string')) \\\n",
        "      .withColumn('location', sl.getItem(2).cast('string')) \\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float')) \\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float')) \\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float')) \\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float')) \\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "\n",
        "try:\n",
        "  results = results \\\n",
        "            .withWatermark('timestamp', '1 seconds')\\\n",
        "            .groupBy('location', window( results.timestamp, '10 seconds', '10 seconds')) \\\n",
        "            .count().alias('count')\\\n",
        "\n",
        "  query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(20)\n",
        "  query.stop()\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  "
      ],
      "metadata": {
        "id": "3mvSOvGtApDp",
        "outputId": "1b1b46c4-e552-45b7-fa2b-cf72da1b4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------+-----+\n",
            "|location|window|count|\n",
            "+--------+------+-----+\n",
            "+--------+------+-----+\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|14857   |{2020-01-01 17:05:20, 2020-01-01 17:05:30}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 196, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 193, in call\n",
            "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
            "  File \"<ipython-input-29-54338f2daaeb>\", line 14, in <lambda>\n",
            "    query = results     .writeStream     .outputMode('complete')     .foreachBatch(lambda df, epoch: df.show(10, False))     .start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 502, in show\n",
            "    print(self._jdf.showString(n, int_truncate, vertical))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o2228.showString.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:929)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext,1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "  line = lines.window(1,1)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:('{}{}'.format(x.split(' ')[2],x.split(' ')[-1][8:10]),x.split(' ')[-3]))\\\n",
        "  .map(lambda x:((x[0]),x[1]))\\\n",
        "  .reduceByKey(lambda a,b: max(a+b))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj",
        "outputId": "99584f4d-9ddc-4de6-c401-15793f5795d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o7128.awaitTerminationOrTimeout.\n",
            ": org.apache.spark.SparkException: An exception was raised by Python:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/util.py\", line 68, in call\n",
            "    r = self.func(t, *rdds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
            "    taken = rdd.take(num + 1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1568, in take\n",
            "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/context.py\", line 1227, in runJob\n",
            "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (314b75525b3e executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-48-7a2e24b44f44>\", line 15, in <lambda>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 163, in max\n",
            "    return _invoke_function_over_column(\"max\", col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 66, in _invoke_function_over_column\n",
            "    return _invoke_function(name, _to_java_column(col))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 43, in _to_java_column\n",
            "    jcol = _create_column_from_name(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 36, in _create_column_from_name\n",
            "    return sc._jvm.functions.col(name)\n",
            "AttributeError: 'NoneType' object has no attribute '_jvm'\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
            "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy22.call(Unknown Source)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-48-7a2e24b44f44>\", line 15, in <lambda>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 163, in max\n",
            "    return _invoke_function_over_column(\"max\", col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 66, in _invoke_function_over_column\n",
            "    return _invoke_function(name, _to_java_column(col))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 43, in _to_java_column\n",
            "    jcol = _create_column_from_name(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 36, in _create_column_from_name\n",
            "    return sc._jvm.functions.col(name)\n",
            "AttributeError: 'NoneType' object has no attribute '_jvm'\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\t... 3 more\n",
            "\n",
            "\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average air quality, for each location. Updated hourly;"
      ],
      "metadata": {
        "id": "niYfjlYZ034R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1]))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rZnFTf83TtQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8d2ff1-b997-4f10-c876-6d5439ab80e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:47\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:48\n",
            "-------------------------------------------\n",
            "('13691 00', 28.824400000000004)\n",
            "('14857 00', 178.77695652173915)\n",
            "('14858 01', 229.77)\n",
            "('10161 00', 117.895)\n",
            "('14858 00', 263.3808695652174)\n",
            "('2332 00', 549.0566666666666)\n",
            "('14857 01', 129.47)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:49\n",
            "-------------------------------------------\n",
            "('14858 01', 212.37227272727276)\n",
            "('2332 01', 412.6426086956522)\n",
            "('10161 01', 101.8625)\n",
            "('13691 01', 23.940833333333334)\n",
            "('14857 01', 110.07318181818181)\n",
            "('10161 02', 76.43)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:50\n",
            "-------------------------------------------\n",
            "('13691 02', 21.760833333333338)\n",
            "('13691 03', 12.6)\n",
            "('2332 02', 314.77149999999995)\n",
            "('14857 02', 73.19250000000001)\n",
            "('14858 02', 157.10095238095238)\n",
            "('10161 02', 75.21333333333332)\n",
            "('19563 02', 46.72)\n",
            "('14857 03', 60.73)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:51\n",
            "-------------------------------------------\n",
            "('13691 03', 10.992727272727274)\n",
            "('10161 03', 73.2675)\n",
            "('14857 03', 60.79549999999999)\n",
            "('14858 03', 133.3675)\n",
            "('2332 03', 319.23500000000007)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:52\n",
            "-------------------------------------------\n",
            "('2332 04', 256.52521739130435)\n",
            "('14857 04', 46.32909090909091)\n",
            "('14858 04', 121.8)\n",
            "('13691 04', 9.926086956521738)\n",
            "('10161 04', 60.775909090909096)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you show any daily and/or weekly patterns to air quality?"
      ],
      "metadata": {
        "id": "kFNEqiRH-o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#location | timestamp | P1\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df_pandas_1=pd.DataFrame({'location':[],'timestamp':[],'pollution':[]})\n",
        "\n",
        "def convert_to_df(rdd):\n",
        "  try:\n",
        "    df = spark.createDataFrame(rdd)\n",
        "    df_pandas = df.toPandas()\n",
        "    df_pandas.columns = ['location', 'timestamp', 'pollution']\n",
        "    df_pandas_1.append(df_pandas, ignore_index = True)\n",
        "    print(df_pandas_1)\n",
        "  except Exception as err:\n",
        "    print(err) \n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "  line = lines.window(1,1)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:(x.split(' ')[2],x.split(' ')[-1],float(x.split(' ')[-3])))\\\n",
        "  .filter(lambda x : x[0] =='13691') \n",
        "\n",
        "  results.pprint()\n",
        "  results.foreachRDD(convert_to_df)\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3AtenvGOGd7",
        "outputId": "fd5ef2bf-72d6-4acf-9f2c-585f93764b24"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-29 10:34:56\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T00:01:21', 19.83)\n",
            "('13691', '2020-01-01T00:03:47', 19.07)\n",
            "('13691', '2020-01-01T00:06:13', 15.73)\n",
            "('13691', '2020-01-01T00:08:40', 18.4)\n",
            "('13691', '2020-01-01T00:11:07', 16.67)\n",
            "('13691', '2020-01-01T00:13:33', 23.73)\n",
            "('13691', '2020-01-01T00:15:59', 39.33)\n",
            "('13691', '2020-01-01T00:18:26', 128.13)\n",
            "('13691', '2020-01-01T00:20:52', 74.9)\n",
            "('13691', '2020-01-01T00:23:18', 33.93)\n",
            "...\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [location, timestamp, pollution]\n",
            "Index: []\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 10:34:57\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T00:30:37', 22.8)\n",
            "('13691', '2020-01-01T00:33:03', 21.3)\n",
            "('13691', '2020-01-01T00:35:29', 31.2)\n",
            "('13691', '2020-01-01T00:37:56', 20.0)\n",
            "('13691', '2020-01-01T00:40:22', 16.6)\n",
            "('13691', '2020-01-01T00:42:48', 15.63)\n",
            "('13691', '2020-01-01T00:45:14', 14.97)\n",
            "('13691', '2020-01-01T00:47:41', 14.57)\n",
            "('13691', '2020-01-01T00:50:07', 14.43)\n",
            "('13691', '2020-01-01T00:52:33', 18.23)\n",
            "...\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [location, timestamp, pollution]\n",
            "Index: []\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 10:34:58\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T01:31:47', 20.97)\n",
            "('13691', '2020-01-01T01:34:14', 20.97)\n",
            "('13691', '2020-01-01T01:36:40', 17.33)\n",
            "('13691', '2020-01-01T01:39:12', 14.9)\n",
            "('13691', '2020-01-01T01:41:43', 11.37)\n",
            "('13691', '2020-01-01T01:44:09', 12.17)\n",
            "('13691', '2020-01-01T01:46:35', 13.5)\n",
            "('13691', '2020-01-01T01:49:01', 11.63)\n",
            "('13691', '2020-01-01T01:51:28', 15.83)\n",
            "('13691', '2020-01-01T01:53:54', 16.4)\n",
            "...\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [location, timestamp, pollution]\n",
            "Index: []\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 10:34:59\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T02:30:30', 11.43)\n",
            "('13691', '2020-01-01T02:32:56', 11.6)\n",
            "('13691', '2020-01-01T02:35:22', 11.97)\n",
            "('13691', '2020-01-01T02:37:49', 9.5)\n",
            "('13691', '2020-01-01T02:40:15', 13.1)\n",
            "('13691', '2020-01-01T02:42:41', 13.37)\n",
            "('13691', '2020-01-01T02:45:08', 11.17)\n",
            "('13691', '2020-01-01T02:47:34', 13.03)\n",
            "('13691', '2020-01-01T02:50:16', 15.23)\n",
            "('13691', '2020-01-01T02:52:42', 13.07)\n",
            "...\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [location, timestamp, pollution]\n",
            "Index: []\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 10:35:00\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T03:32:01', 10.3)\n",
            "('13691', '2020-01-01T03:34:27', 8.83)\n",
            "('13691', '2020-01-01T03:36:53', 9.33)\n",
            "('13691', '2020-01-01T03:39:20', 12.9)\n",
            "('13691', '2020-01-01T03:41:49', 11.43)\n",
            "('13691', '2020-01-01T03:44:15', 10.83)\n",
            "('13691', '2020-01-01T03:51:06', 9.73)\n",
            "('13691', '2020-01-01T03:53:52', 12.43)\n",
            "('13691', '2020-01-01T03:56:21', 10.77)\n",
            "('13691', '2020-01-01T03:58:51', 12.3)\n",
            "...\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [location, timestamp, pollution]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp"
      ],
      "metadata": {
        "id": "D9vaOwIkBFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, count\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('StructuredWebLogExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "  \n",
        "sl = split( lines.value, ' ')\n",
        "table = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string'))\\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string'))\\\n",
        "      .withColumn('location', sl.getItem(2).cast('string'))\\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float'))\\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float'))\\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float'))\\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float'))\\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "try:\n",
        "\n",
        "  results = table \\\n",
        "            .groupBy('location') \\\n",
        "            .count()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "'''\n",
        "x = results.select('timestamp').toList()\n",
        "y = results.select('P2').toList()\n",
        "plt.bar(x,y)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "fuA_-gI1-lbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}