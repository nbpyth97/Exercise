{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2022-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbpyth97/Exercise/blob/master/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "30th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi",
        "outputId": "5bb714e7-10d8-4f4f-fe9c-6e56fea40591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "outputId": "f15ea2a3-82c5-46b2-fece-84e61f5354bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form",
        "outputId": "ce9de116-3c89-4f45-c4f0-d3af424c2c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting /tmp/kraft-combined-logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1",
        "outputId": "d53c7640-e16d-448e-cb0c-e9fc22f37b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(5, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "#top5 = lines.groupBy(window(lines.timestamp, '24 seconds', '24 seconds'), 'location') \\\n",
        "#       .count()\n",
        "\n",
        "query = lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode('append') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "query.awaitTermination(30)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5fc10bf840c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumpBatchDF\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"Stop this streaming query.\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "\n",
        "sl = split(lines.value, ' ')\n",
        "\n",
        "results = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string')) \\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string')) \\\n",
        "      .withColumn('location', sl.getItem(2).cast('string')) \\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float')) \\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float')) \\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float')) \\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float')) \\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "\n",
        "try:\n",
        "  results = results \\\n",
        "            .withWatermark('timestamp', '1 seconds')\\\n",
        "            .groupBy('location', window( results.timestamp, '10 seconds', '10 seconds')) \\\n",
        "            .count().alias('count')\\\n",
        "\n",
        "  query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(20)\n",
        "  query.stop()\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  "
      ],
      "metadata": {
        "id": "3mvSOvGtApDp",
        "outputId": "1b1b46c4-e552-45b7-fa2b-cf72da1b4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------+-----+\n",
            "|location|window|count|\n",
            "+--------+------+-----+\n",
            "+--------+------+-----+\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|14857   |{2020-01-01 17:05:20, 2020-01-01 17:05:30}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 196, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 193, in call\n",
            "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
            "  File \"<ipython-input-29-54338f2daaeb>\", line 14, in <lambda>\n",
            "    query = results     .writeStream     .outputMode('complete')     .foreachBatch(lambda df, epoch: df.show(10, False))     .start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 502, in show\n",
            "    print(self._jdf.showString(n, int_truncate, vertical))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o2228.showString.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:929)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the time of day with the poorest air quality, for each location. Updated daily"
      ],
      "metadata": {
        "id": "Cs_uSpfg6Vmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext,1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "  line = lines.window(24,24)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2],x.split(' ')[-1][8:10]),(x.split(' ')[-3],x.split(' ')[-1])))\\\n",
        "  .reduceByKey(max)\n",
        "  #.reduceByKey(lambda a,b: max(a+b))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(72)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj",
        "outputId": "26b29cb1-9542-4ab0-adca-42c51eadcf11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-05-02 13:35:54\n",
            "-------------------------------------------\n",
            "(('13691', '01'), ('97.07', '2020-01-01T18:45:09'))\n",
            "(('14858', '01'), ('99.3', '2020-01-01T22:44:53'))\n",
            "(('2332', '01'), ('99.8', '2020-01-01T17:04:04'))\n",
            "(('10161', '01'), ('99.67', '2020-01-01T17:00:58'))\n",
            "(('14857', '01'), ('99.57', '2020-01-01T12:26:59'))\n",
            "(('19563', '01'), ('9.55', '2020-01-01T13:17:57'))\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 13:36:18\n",
            "-------------------------------------------\n",
            "(('14858', '01'), ('95.27', '2020-01-01T23:19:34'))\n",
            "(('13691', '01'), ('37.5', '2020-01-01T23:27:37'))\n",
            "(('2332', '01'), ('233.9', '2020-01-01T23:43:42'))\n",
            "(('2332', '02'), ('87.1', '2020-01-02T04:12:52'))\n",
            "(('13691', '02'), ('9.93', '2020-01-02T06:20:19'))\n",
            "(('14858', '02'), ('99.95', '2020-01-02T15:54:02'))\n",
            "(('19563', '01'), ('47.9', '2020-01-01T23:50:11'))\n",
            "(('10161', '01'), ('42.1', '2020-01-01T23:55:51'))\n",
            "(('14857', '01'), ('70.2', '2020-01-01T23:20:40'))\n",
            "(('14857', '02'), ('99.93', '2020-01-02T17:36:48'))\n",
            "...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average air quality, for each location. Updated hourly;"
      ],
      "metadata": {
        "id": "niYfjlYZ034R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1]))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rZnFTf83TtQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d46cddf-cf44-48e1-e4da-fe6d2da9dd51"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-05-02 14:11:16\n",
            "-------------------------------------------\n",
            "('13691 00', 17.94)\n",
            "('14857 00', 263.26)\n",
            "('10161 00', 110.88799999999999)\n",
            "('14858 00', 314.0)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 14:11:17\n",
            "-------------------------------------------\n",
            "('14857 00', 160.99105263157895)\n",
            "('13691 00', 31.545500000000004)\n",
            "('14858 01', 236.288)\n",
            "('14858 00', 252.7242105263158)\n",
            "('10161 00', 119.73894736842105)\n",
            "('2332 00', 549.0566666666666)\n",
            "('14857 01', 125.78)\n",
            "('2332 01', 465.96000000000004)\n",
            "('10161 01', 109.954)\n",
            "('13691 01', 39.836)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 14:11:18\n",
            "-------------------------------------------\n",
            "('14858 01', 206.6955555555556)\n",
            "('13691 02', 43.934000000000005)\n",
            "('14857 01', 106.78777777777776)\n",
            "('2332 01', 397.8322222222222)\n",
            "('10161 01', 99.73315789473683)\n",
            "('13691 01', 19.757894736842104)\n",
            "('10161 02', 73.53200000000001)\n",
            "('2332 02', 383.788)\n",
            "('14857 02', 80.625)\n",
            "('14858 02', 163.39)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 14:11:19\n",
            "-------------------------------------------\n",
            "('13691 02', 15.925789473684212)\n",
            "('13691 03', 11.81)\n",
            "('19563 02', 46.72)\n",
            "('14857 02', 71.33437500000001)\n",
            "('10161 02', 75.69449999999999)\n",
            "('2332 02', 291.76599999999996)\n",
            "('14858 02', 155.62117647058824)\n",
            "('14857 03', 61.846000000000004)\n",
            "('10161 03', 77.528)\n",
            "('14858 03', 135.825)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 14:11:20\n",
            "-------------------------------------------\n",
            "('13691 03', 10.798823529411763)\n",
            "('2332 04', 221.385)\n",
            "('14857 04', 49.0675)\n",
            "('14857 03', 60.463125000000005)\n",
            "('14858 03', 132.75312499999998)\n",
            "('10161 03', 72.14631578947369)\n",
            "('2332 03', 319.23500000000007)\n",
            "('14858 04', 126.546)\n",
            "('13691 04', 10.008)\n",
            "('10161 04', 65.715)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you show any daily and/or weekly patterns to air quality?"
      ],
      "metadata": {
        "id": "kFNEqiRH-o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#location | timestamp | P1\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "array_x = []\n",
        "\n",
        "def convert_to_df(rdd):\n",
        "  global array_x\n",
        "  try:\n",
        "    \n",
        "    df = spark.createDataFrame(rdd)\n",
        "    df_pandas = df.toPandas()\n",
        "    df_pandas.columns = ['location', 'timestamp', 'pollution']\n",
        "    '''\n",
        "    x = df_pandas['timestamp']\n",
        "    y = df_pandas['pollution']\n",
        "    plt.bar(x,y)\n",
        "    plt.plot()\n",
        "    plt.xlabel(\"Number of Days\")\n",
        "    plt.ylabel(\"Pollution\")\n",
        "    plt.title(\"Pollution in location 13691\")\n",
        "    plt.legend()\n",
        "    '''\n",
        "    if not array_x: \n",
        "      print(' A LISTA ESTÁ VAZIA!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "      array_x=[df_pandas['timestamp']]\n",
        "    else:\n",
        "      array_x.append(df_pandas['timestamp'])\n",
        "    print(array_x)\n",
        "    print(df_pandas)\n",
        "  except Exception as err:\n",
        "    print(err) \n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "  line = lines.window(1,1)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:(x.split(' ')[2],x.split(' ')[-1][8:10],float(x.split(' ')[-3])))\\\n",
        "  .filter(lambda x : x[0] == '13691')\n",
        "\n",
        "  results.pprint()\n",
        "  results.foreachRDD(convert_to_df)\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(3)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3AtenvGOGd7",
        "outputId": "b308116e-3fad-4ce0-fad7-1626a59b250b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-05-02 15:04:20\n",
            "-------------------------------------------\n",
            "\n",
            "RDD is empty\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 15:04:21\n",
            "-------------------------------------------\n",
            "('13691', '01', 19.83)\n",
            "('13691', '01', 19.07)\n",
            "('13691', '01', 15.73)\n",
            "('13691', '01', 18.4)\n",
            "('13691', '01', 16.67)\n",
            "('13691', '01', 23.73)\n",
            "('13691', '01', 39.33)\n",
            "('13691', '01', 128.13)\n",
            "('13691', '01', 74.9)\n",
            "('13691', '01', 33.93)\n",
            "...\n",
            "\n",
            " A LISTA ESTÁ VAZIA!!!!!!!!!!!!!!!!!!!!!!!\n",
            "[0     01\n",
            "1     01\n",
            "2     01\n",
            "3     01\n",
            "4     01\n",
            "5     01\n",
            "6     01\n",
            "7     01\n",
            "8     01\n",
            "9     01\n",
            "10    01\n",
            "11    01\n",
            "12    01\n",
            "13    01\n",
            "14    01\n",
            "15    01\n",
            "16    01\n",
            "17    01\n",
            "18    01\n",
            "19    01\n",
            "20    01\n",
            "21    01\n",
            "22    01\n",
            "Name: timestamp, dtype: object]\n",
            "   location timestamp  pollution\n",
            "0     13691        01      19.83\n",
            "1     13691        01      19.07\n",
            "2     13691        01      15.73\n",
            "3     13691        01      18.40\n",
            "4     13691        01      16.67\n",
            "5     13691        01      23.73\n",
            "6     13691        01      39.33\n",
            "7     13691        01     128.13\n",
            "8     13691        01      74.90\n",
            "9     13691        01      33.93\n",
            "10    13691        01      24.43\n",
            "11    13691        01      21.33\n",
            "12    13691        01      22.80\n",
            "13    13691        01      21.30\n",
            "14    13691        01      31.20\n",
            "15    13691        01      20.00\n",
            "16    13691        01      16.60\n",
            "17    13691        01      15.63\n",
            "18    13691        01      14.97\n",
            "19    13691        01      14.57\n",
            "20    13691        01      14.43\n",
            "21    13691        01      18.23\n",
            "22    13691        01      18.87\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 15:04:22\n",
            "-------------------------------------------\n",
            "('13691', '01', 40.83)\n",
            "('13691', '01', 35.7)\n",
            "('13691', '01', 46.37)\n",
            "('13691', '01', 35.47)\n",
            "('13691', '01', 49.27)\n",
            "('13691', '01', 35.97)\n",
            "('13691', '01', 32.1)\n",
            "('13691', '01', 36.47)\n",
            "('13691', '01', 36.9)\n",
            "('13691', '01', 29.7)\n",
            "...\n",
            "\n",
            "[0     01\n",
            "1     01\n",
            "2     01\n",
            "3     01\n",
            "4     01\n",
            "5     01\n",
            "6     01\n",
            "7     01\n",
            "8     01\n",
            "9     01\n",
            "10    01\n",
            "11    01\n",
            "12    01\n",
            "13    01\n",
            "14    01\n",
            "15    01\n",
            "16    01\n",
            "17    01\n",
            "18    01\n",
            "19    01\n",
            "20    01\n",
            "21    01\n",
            "22    01\n",
            "Name: timestamp, dtype: object, 0     01\n",
            "1     01\n",
            "2     01\n",
            "3     01\n",
            "4     01\n",
            "5     01\n",
            "6     01\n",
            "7     01\n",
            "8     01\n",
            "9     01\n",
            "10    01\n",
            "11    01\n",
            "12    01\n",
            "13    01\n",
            "14    01\n",
            "15    01\n",
            "16    01\n",
            "17    01\n",
            "18    01\n",
            "19    01\n",
            "20    01\n",
            "21    01\n",
            "22    01\n",
            "23    01\n",
            "Name: timestamp, dtype: object]\n",
            "   location timestamp  pollution\n",
            "0     13691        01      40.83\n",
            "1     13691        01      35.70\n",
            "2     13691        01      46.37\n",
            "3     13691        01      35.47\n",
            "4     13691        01      49.27\n",
            "5     13691        01      35.97\n",
            "6     13691        01      32.10\n",
            "7     13691        01      36.47\n",
            "8     13691        01      36.90\n",
            "9     13691        01      29.70\n",
            "10    13691        01      25.33\n",
            "11    13691        01      21.63\n",
            "12    13691        01      20.43\n",
            "13    13691        01      24.47\n",
            "14    13691        01      20.97\n",
            "15    13691        01      20.97\n",
            "16    13691        01      17.33\n",
            "17    13691        01      14.90\n",
            "18    13691        01      11.37\n",
            "19    13691        01      12.17\n",
            "20    13691        01      13.50\n",
            "21    13691        01      11.63\n",
            "22    13691        01      15.83\n",
            "23    13691        01      16.40\n",
            "-------------------------------------------\n",
            "Time: 2022-05-02 15:04:23\n",
            "-------------------------------------------\n",
            "('13691', '01', 13.13)\n",
            "('13691', '01', 12.27)\n",
            "('13691', '01', 27.1)\n",
            "('13691', '01', 26.13)\n",
            "('13691', '01', 46.67)\n",
            "('13691', '01', 56.6)\n",
            "('13691', '01', 63.17)\n",
            "('13691', '01', 79.67)\n",
            "('13691', '01', 16.23)\n",
            "('13691', '01', 16.2)\n",
            "...\n",
            "\n",
            "[0     01\n",
            "1     01\n",
            "2     01\n",
            "3     01\n",
            "4     01\n",
            "5     01\n",
            "6     01\n",
            "7     01\n",
            "8     01\n",
            "9     01\n",
            "10    01\n",
            "11    01\n",
            "12    01\n",
            "13    01\n",
            "14    01\n",
            "15    01\n",
            "16    01\n",
            "17    01\n",
            "18    01\n",
            "19    01\n",
            "20    01\n",
            "21    01\n",
            "22    01\n",
            "Name: timestamp, dtype: object, 0     01\n",
            "1     01\n",
            "2     01\n",
            "3     01\n",
            "4     01\n",
            "5     01\n",
            "6     01\n",
            "7     01\n",
            "8     01\n",
            "9     01\n",
            "10    01\n",
            "11    01\n",
            "12    01\n",
            "13    01\n",
            "14    01\n",
            "15    01\n",
            "16    01\n",
            "17    01\n",
            "18    01\n",
            "19    01\n",
            "20    01\n",
            "21    01\n",
            "22    01\n",
            "23    01\n",
            "Name: timestamp, dtype: object, 0     01\n",
            "1     01\n",
            "2     01\n",
            "3     01\n",
            "4     01\n",
            "5     01\n",
            "6     01\n",
            "7     01\n",
            "8     01\n",
            "9     01\n",
            "10    01\n",
            "11    01\n",
            "12    01\n",
            "13    01\n",
            "14    01\n",
            "15    01\n",
            "16    01\n",
            "17    01\n",
            "18    01\n",
            "19    01\n",
            "20    01\n",
            "21    01\n",
            "22    01\n",
            "23    01\n",
            "24    01\n",
            "Name: timestamp, dtype: object]\n",
            "   location timestamp  pollution\n",
            "0     13691        01      13.13\n",
            "1     13691        01      12.27\n",
            "2     13691        01      27.10\n",
            "3     13691        01      26.13\n",
            "4     13691        01      46.67\n",
            "5     13691        01      56.60\n",
            "6     13691        01      63.17\n",
            "7     13691        01      79.67\n",
            "8     13691        01      16.23\n",
            "9     13691        01      16.20\n",
            "10    13691        01      11.70\n",
            "11    13691        01      11.43\n",
            "12    13691        01       9.93\n",
            "13    13691        01      11.13\n",
            "14    13691        01      11.43\n",
            "15    13691        01      11.60\n",
            "16    13691        01      11.97\n",
            "17    13691        01       9.50\n",
            "18    13691        01      13.10\n",
            "19    13691        01      13.37\n",
            "20    13691        01      11.17\n",
            "21    13691        01      13.03\n",
            "22    13691        01      15.23\n",
            "23    13691        01      13.07\n",
            "24    13691        01      11.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    '''\n",
        "    plt.bar(x,y)\n",
        "    plt.plot()\n",
        "    plt.xlabel(\"Number of Days\")\n",
        "    plt.ylabel(\"Pollution\")\n",
        "    plt.title(\"Pollution in location 13691\")\n",
        "    plt.legend()\n",
        "    '''"
      ],
      "metadata": {
        "id": "QNwcvJ8xHOJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp"
      ],
      "metadata": {
        "id": "D9vaOwIkBFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, count\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('StructuredWebLogExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "  \n",
        "sl = split( lines.value, ' ')\n",
        "table = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string'))\\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string'))\\\n",
        "      .withColumn('location', sl.getItem(2).cast('string'))\\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float'))\\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float'))\\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float'))\\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float'))\\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "try:\n",
        "\n",
        "  results = table \\\n",
        "            .groupBy('location') \\\n",
        "            .count()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "'''\n",
        "x = results.select('timestamp').toList()\n",
        "y = results.select('P2').toList()\n",
        "plt.bar(x,y)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "fuA_-gI1-lbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}