{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2022-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbpyth97/Exercise/blob/master/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "30th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi",
        "outputId": "dcd954ad-08eb-45d1-f343-a420b65978aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "outputId": "e0402a37-35d1-4ec0-9970-57811d35d177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 31 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 14.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form",
        "outputId": "ce9de116-3c89-4f45-c4f0-d3af424c2c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting /tmp/kraft-combined-logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1",
        "outputId": "d53c7640-e16d-448e-cb0c-e9fc22f37b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(5, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "#top5 = lines.groupBy(window(lines.timestamp, '24 seconds', '24 seconds'), 'location') \\\n",
        "#       .count()\n",
        "\n",
        "query = lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode('append') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "query.awaitTermination(30)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5fc10bf840c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumpBatchDF\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"Stop this streaming query.\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "\n",
        "sl = split(lines.value, ' ')\n",
        "\n",
        "results = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string')) \\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string')) \\\n",
        "      .withColumn('location', sl.getItem(2).cast('string')) \\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float')) \\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float')) \\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float')) \\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float')) \\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "\n",
        "try:\n",
        "  results = results \\\n",
        "            .withWatermark('timestamp', '1 seconds')\\\n",
        "            .groupBy('location', window( results.timestamp, '10 seconds', '10 seconds')) \\\n",
        "            .count().alias('count')\\\n",
        "\n",
        "  query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(20)\n",
        "  query.stop()\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  "
      ],
      "metadata": {
        "id": "3mvSOvGtApDp",
        "outputId": "1b1b46c4-e552-45b7-fa2b-cf72da1b4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------+-----+\n",
            "|location|window|count|\n",
            "+--------+------+-----+\n",
            "+--------+------+-----+\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|14857   |{2020-01-01 17:05:20, 2020-01-01 17:05:30}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 196, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 193, in call\n",
            "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
            "  File \"<ipython-input-29-54338f2daaeb>\", line 14, in <lambda>\n",
            "    query = results     .writeStream     .outputMode('complete')     .foreachBatch(lambda df, epoch: df.show(10, False))     .start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 502, in show\n",
            "    print(self._jdf.showString(n, int_truncate, vertical))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o2228.showString.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:929)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext,1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "  line = lines.window(1,1)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:('{}{}'.format(x.split(' ')[2],x.split(' ')[-1][8:10]),x.split(' ')[-3]))\\\n",
        "  .map(lambda x:((x[0]),x[1]))\\\n",
        "  .reduceByKey(lambda a,b: max(a+b))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj",
        "outputId": "99584f4d-9ddc-4de6-c401-15793f5795d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o7128.awaitTerminationOrTimeout.\n",
            ": org.apache.spark.SparkException: An exception was raised by Python:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/util.py\", line 68, in call\n",
            "    r = self.func(t, *rdds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
            "    taken = rdd.take(num + 1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1568, in take\n",
            "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/context.py\", line 1227, in runJob\n",
            "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (314b75525b3e executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-48-7a2e24b44f44>\", line 15, in <lambda>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 163, in max\n",
            "    return _invoke_function_over_column(\"max\", col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 66, in _invoke_function_over_column\n",
            "    return _invoke_function(name, _to_java_column(col))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 43, in _to_java_column\n",
            "    jcol = _create_column_from_name(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 36, in _create_column_from_name\n",
            "    return sc._jvm.functions.col(name)\n",
            "AttributeError: 'NoneType' object has no attribute '_jvm'\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
            "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy22.call(Unknown Source)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-48-7a2e24b44f44>\", line 15, in <lambda>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 163, in max\n",
            "    return _invoke_function_over_column(\"max\", col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 66, in _invoke_function_over_column\n",
            "    return _invoke_function(name, _to_java_column(col))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 43, in _to_java_column\n",
            "    jcol = _create_column_from_name(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 36, in _create_column_from_name\n",
            "    return sc._jvm.functions.col(name)\n",
            "AttributeError: 'NoneType' object has no attribute '_jvm'\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\t... 3 more\n",
            "\n",
            "\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average air quality, for each location. Updated hourly;"
      ],
      "metadata": {
        "id": "niYfjlYZ034R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1]))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rZnFTf83TtQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8d2ff1-b997-4f10-c876-6d5439ab80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:47\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:48\n",
            "-------------------------------------------\n",
            "('13691 00', 28.824400000000004)\n",
            "('14857 00', 178.77695652173915)\n",
            "('14858 01', 229.77)\n",
            "('10161 00', 117.895)\n",
            "('14858 00', 263.3808695652174)\n",
            "('2332 00', 549.0566666666666)\n",
            "('14857 01', 129.47)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:49\n",
            "-------------------------------------------\n",
            "('14858 01', 212.37227272727276)\n",
            "('2332 01', 412.6426086956522)\n",
            "('10161 01', 101.8625)\n",
            "('13691 01', 23.940833333333334)\n",
            "('14857 01', 110.07318181818181)\n",
            "('10161 02', 76.43)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:50\n",
            "-------------------------------------------\n",
            "('13691 02', 21.760833333333338)\n",
            "('13691 03', 12.6)\n",
            "('2332 02', 314.77149999999995)\n",
            "('14857 02', 73.19250000000001)\n",
            "('14858 02', 157.10095238095238)\n",
            "('10161 02', 75.21333333333332)\n",
            "('19563 02', 46.72)\n",
            "('14857 03', 60.73)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:51\n",
            "-------------------------------------------\n",
            "('13691 03', 10.992727272727274)\n",
            "('10161 03', 73.2675)\n",
            "('14857 03', 60.79549999999999)\n",
            "('14858 03', 133.3675)\n",
            "('2332 03', 319.23500000000007)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:52\n",
            "-------------------------------------------\n",
            "('2332 04', 256.52521739130435)\n",
            "('14857 04', 46.32909090909091)\n",
            "('14858 04', 121.8)\n",
            "('13691 04', 9.926086956521738)\n",
            "('10161 04', 60.775909090909096)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you show any daily and/or weekly patterns to air quality?"
      ],
      "metadata": {
        "id": "kFNEqiRH-o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#location | timestamp | P1\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "def convert_to_df(rdd):\n",
        "  try:\n",
        "    df = spark.createDataFrame(rdd)\n",
        "    df_pandas = df.toPandas()\n",
        "    df_pandas.columns = ['location', 'timestamp', 'pollution']\n",
        "    x = df_pandas['timestamp']\n",
        "    y = df_pandas['pollution']\n",
        "    print(x)\n",
        "    print(y)\n",
        "    plt.bar(x,y)\n",
        "    plt.plot()\n",
        "\n",
        "  except Exception as err:\n",
        "    print(err) \n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "  line = lines.window(1,1)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:(x.split(' ')[2],x.split(' ')[-1],float(x.split(' ')[-3])))\\\n",
        "  .filter(lambda x : x[0] =='13691')\n",
        "\n",
        "  results.pprint()\n",
        "  results.foreachRDD(convert_to_df)\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A3AtenvGOGd7",
        "outputId": "78190286-b600-44a0-d443-1298022ae010"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:06:37\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T00:01:21', 19.83)\n",
            "('13691', '2020-01-01T00:03:47', 19.07)\n",
            "\n",
            "0    2020-01-01T00:01:21\n",
            "1    2020-01-01T00:03:47\n",
            "Name: timestamp, dtype: object\n",
            "0    19.83\n",
            "1    19.07\n",
            "Name: pollution, dtype: float64\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:06:38\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T00:06:13', 15.73)\n",
            "('13691', '2020-01-01T00:08:40', 18.4)\n",
            "('13691', '2020-01-01T00:11:07', 16.67)\n",
            "('13691', '2020-01-01T00:13:33', 23.73)\n",
            "('13691', '2020-01-01T00:15:59', 39.33)\n",
            "('13691', '2020-01-01T00:18:26', 128.13)\n",
            "('13691', '2020-01-01T00:20:52', 74.9)\n",
            "('13691', '2020-01-01T00:23:18', 33.93)\n",
            "('13691', '2020-01-01T00:25:44', 24.43)\n",
            "('13691', '2020-01-01T00:28:10', 21.33)\n",
            "...\n",
            "\n",
            "0     2020-01-01T00:06:13\n",
            "1     2020-01-01T00:08:40\n",
            "2     2020-01-01T00:11:07\n",
            "3     2020-01-01T00:13:33\n",
            "4     2020-01-01T00:15:59\n",
            "5     2020-01-01T00:18:26\n",
            "6     2020-01-01T00:20:52\n",
            "7     2020-01-01T00:23:18\n",
            "8     2020-01-01T00:25:44\n",
            "9     2020-01-01T00:28:10\n",
            "10    2020-01-01T00:30:37\n",
            "11    2020-01-01T00:33:03\n",
            "12    2020-01-01T00:35:29\n",
            "13    2020-01-01T00:37:56\n",
            "14    2020-01-01T00:40:22\n",
            "15    2020-01-01T00:42:48\n",
            "16    2020-01-01T00:45:14\n",
            "17    2020-01-01T00:47:41\n",
            "18    2020-01-01T00:50:07\n",
            "19    2020-01-01T00:52:33\n",
            "20    2020-01-01T00:54:59\n",
            "21    2020-01-01T00:57:25\n",
            "22    2020-01-01T00:59:52\n",
            "23    2020-01-01T01:02:18\n",
            "Name: timestamp, dtype: object\n",
            "0      15.73\n",
            "1      18.40\n",
            "2      16.67\n",
            "3      23.73\n",
            "4      39.33\n",
            "5     128.13\n",
            "6      74.90\n",
            "7      33.93\n",
            "8      24.43\n",
            "9      21.33\n",
            "10     22.80\n",
            "11     21.30\n",
            "12     31.20\n",
            "13     20.00\n",
            "14     16.60\n",
            "15     15.63\n",
            "16     14.97\n",
            "17     14.57\n",
            "18     14.43\n",
            "19     18.23\n",
            "20     18.87\n",
            "21     40.83\n",
            "22     35.70\n",
            "23     46.37\n",
            "Name: pollution, dtype: float64\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:06:39\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T01:04:44', 35.47)\n",
            "('13691', '2020-01-01T01:07:10', 49.27)\n",
            "('13691', '2020-01-01T01:09:38', 35.97)\n",
            "('13691', '2020-01-01T01:12:05', 32.1)\n",
            "('13691', '2020-01-01T01:14:31', 36.47)\n",
            "('13691', '2020-01-01T01:16:57', 36.9)\n",
            "('13691', '2020-01-01T01:19:24', 29.7)\n",
            "('13691', '2020-01-01T01:21:50', 25.33)\n",
            "('13691', '2020-01-01T01:24:17', 21.63)\n",
            "('13691', '2020-01-01T01:26:43', 20.43)\n",
            "...\n",
            "\n",
            "0     2020-01-01T01:04:44\n",
            "1     2020-01-01T01:07:10\n",
            "2     2020-01-01T01:09:38\n",
            "3     2020-01-01T01:12:05\n",
            "4     2020-01-01T01:14:31\n",
            "5     2020-01-01T01:16:57\n",
            "6     2020-01-01T01:19:24\n",
            "7     2020-01-01T01:21:50\n",
            "8     2020-01-01T01:24:17\n",
            "9     2020-01-01T01:26:43\n",
            "10    2020-01-01T01:29:09\n",
            "11    2020-01-01T01:31:47\n",
            "12    2020-01-01T01:34:14\n",
            "13    2020-01-01T01:36:40\n",
            "14    2020-01-01T01:39:12\n",
            "15    2020-01-01T01:41:43\n",
            "16    2020-01-01T01:44:09\n",
            "17    2020-01-01T01:46:35\n",
            "18    2020-01-01T01:49:01\n",
            "19    2020-01-01T01:51:28\n",
            "20    2020-01-01T01:53:54\n",
            "21    2020-01-01T01:56:20\n",
            "22    2020-01-01T01:58:46\n",
            "23    2020-01-01T02:01:12\n",
            "24    2020-01-01T02:03:39\n",
            "Name: timestamp, dtype: object\n",
            "0     35.47\n",
            "1     49.27\n",
            "2     35.97\n",
            "3     32.10\n",
            "4     36.47\n",
            "5     36.90\n",
            "6     29.70\n",
            "7     25.33\n",
            "8     21.63\n",
            "9     20.43\n",
            "10    24.47\n",
            "11    20.97\n",
            "12    20.97\n",
            "13    17.33\n",
            "14    14.90\n",
            "15    11.37\n",
            "16    12.17\n",
            "17    13.50\n",
            "18    11.63\n",
            "19    15.83\n",
            "20    16.40\n",
            "21    13.13\n",
            "22    12.27\n",
            "23    27.10\n",
            "24    26.13\n",
            "Name: pollution, dtype: float64\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:06:40\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T02:06:05', 46.67)\n",
            "('13691', '2020-01-01T02:08:31', 56.6)\n",
            "('13691', '2020-01-01T02:10:57', 63.17)\n",
            "('13691', '2020-01-01T02:13:24', 79.67)\n",
            "('13691', '2020-01-01T02:15:50', 16.23)\n",
            "('13691', '2020-01-01T02:18:16', 16.2)\n",
            "('13691', '2020-01-01T02:20:45', 11.7)\n",
            "('13691', '2020-01-01T02:23:11', 11.43)\n",
            "('13691', '2020-01-01T02:25:37', 9.93)\n",
            "('13691', '2020-01-01T02:28:04', 11.13)\n",
            "...\n",
            "\n",
            "0     2020-01-01T02:06:05\n",
            "1     2020-01-01T02:08:31\n",
            "2     2020-01-01T02:10:57\n",
            "3     2020-01-01T02:13:24\n",
            "4     2020-01-01T02:15:50\n",
            "5     2020-01-01T02:18:16\n",
            "6     2020-01-01T02:20:45\n",
            "7     2020-01-01T02:23:11\n",
            "8     2020-01-01T02:25:37\n",
            "9     2020-01-01T02:28:04\n",
            "10    2020-01-01T02:30:30\n",
            "11    2020-01-01T02:32:56\n",
            "12    2020-01-01T02:35:22\n",
            "13    2020-01-01T02:37:49\n",
            "14    2020-01-01T02:40:15\n",
            "15    2020-01-01T02:42:41\n",
            "16    2020-01-01T02:45:08\n",
            "17    2020-01-01T02:47:34\n",
            "18    2020-01-01T02:50:16\n",
            "19    2020-01-01T02:52:42\n",
            "20    2020-01-01T02:55:08\n",
            "21    2020-01-01T02:57:34\n",
            "22    2020-01-01T03:00:01\n",
            "23    2020-01-01T03:02:27\n",
            "Name: timestamp, dtype: object\n",
            "0     46.67\n",
            "1     56.60\n",
            "2     63.17\n",
            "3     79.67\n",
            "4     16.23\n",
            "5     16.20\n",
            "6     11.70\n",
            "7     11.43\n",
            "8      9.93\n",
            "9     11.13\n",
            "10    11.43\n",
            "11    11.60\n",
            "12    11.97\n",
            "13     9.50\n",
            "14    13.10\n",
            "15    13.37\n",
            "16    11.17\n",
            "17    13.03\n",
            "18    15.23\n",
            "19    13.07\n",
            "20    11.30\n",
            "21    11.53\n",
            "22    12.60\n",
            "23    13.00\n",
            "Name: pollution, dtype: float64\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:06:41\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T03:04:53', 10.0)\n",
            "('13691', '2020-01-01T03:07:19', 10.33)\n",
            "('13691', '2020-01-01T03:09:45', 12.03)\n",
            "('13691', '2020-01-01T03:12:12', 12.9)\n",
            "('13691', '2020-01-01T03:14:38', 10.3)\n",
            "('13691', '2020-01-01T03:17:04', 9.73)\n",
            "('13691', '2020-01-01T03:19:30', 10.37)\n",
            "('13691', '2020-01-01T03:21:56', 10.8)\n",
            "('13691', '2020-01-01T03:24:23', 12.23)\n",
            "('13691', '2020-01-01T03:27:08', 10.57)\n",
            "...\n",
            "\n",
            "0     2020-01-01T03:04:53\n",
            "1     2020-01-01T03:07:19\n",
            "2     2020-01-01T03:09:45\n",
            "3     2020-01-01T03:12:12\n",
            "4     2020-01-01T03:14:38\n",
            "5     2020-01-01T03:17:04\n",
            "6     2020-01-01T03:19:30\n",
            "7     2020-01-01T03:21:56\n",
            "8     2020-01-01T03:24:23\n",
            "9     2020-01-01T03:27:08\n",
            "10    2020-01-01T03:29:35\n",
            "11    2020-01-01T03:32:01\n",
            "12    2020-01-01T03:34:27\n",
            "13    2020-01-01T03:36:53\n",
            "14    2020-01-01T03:39:20\n",
            "15    2020-01-01T03:41:49\n",
            "16    2020-01-01T03:44:15\n",
            "17    2020-01-01T03:51:06\n",
            "18    2020-01-01T03:53:52\n",
            "19    2020-01-01T03:56:21\n",
            "20    2020-01-01T03:58:51\n",
            "21    2020-01-01T04:01:20\n",
            "22    2020-01-01T04:03:46\n",
            "Name: timestamp, dtype: object\n",
            "0     10.00\n",
            "1     10.33\n",
            "2     12.03\n",
            "3     12.90\n",
            "4     10.30\n",
            "5      9.73\n",
            "6     10.37\n",
            "7     10.80\n",
            "8     12.23\n",
            "9     10.57\n",
            "10    10.73\n",
            "11    10.30\n",
            "12     8.83\n",
            "13     9.33\n",
            "14    12.90\n",
            "15    11.43\n",
            "16    10.83\n",
            "17     9.73\n",
            "18    12.43\n",
            "19    10.77\n",
            "20    12.30\n",
            "21     9.00\n",
            "22    10.90\n",
            "Name: pollution, dtype: float64\n",
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:06:42\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T04:06:17', 10.17)\n",
            "('13691', '2020-01-01T04:09:34', 10.07)\n",
            "('13691', '2020-01-01T04:12:07', 9.9)\n",
            "('13691', '2020-01-01T04:14:33', 10.3)\n",
            "('13691', '2020-01-01T04:17:00', 10.83)\n",
            "('13691', '2020-01-01T04:19:26', 9.8)\n",
            "('13691', '2020-01-01T04:21:52', 10.2)\n",
            "('13691', '2020-01-01T04:24:18', 8.63)\n",
            "('13691', '2020-01-01T04:26:45', 9.77)\n",
            "('13691', '2020-01-01T04:29:11', 10.37)\n",
            "...\n",
            "\n",
            "0     2020-01-01T04:06:17\n",
            "1     2020-01-01T04:09:34\n",
            "2     2020-01-01T04:12:07\n",
            "3     2020-01-01T04:14:33\n",
            "4     2020-01-01T04:17:00\n",
            "5     2020-01-01T04:19:26\n",
            "6     2020-01-01T04:21:52\n",
            "7     2020-01-01T04:24:18\n",
            "8     2020-01-01T04:26:45\n",
            "9     2020-01-01T04:29:11\n",
            "10    2020-01-01T04:31:37\n",
            "11    2020-01-01T04:34:04\n",
            "12    2020-01-01T04:36:30\n",
            "13    2020-01-01T04:38:56\n",
            "14    2020-01-01T04:41:22\n",
            "15    2020-01-01T04:43:48\n",
            "16    2020-01-01T04:46:15\n",
            "17    2020-01-01T04:48:41\n",
            "18    2020-01-01T04:51:07\n",
            "19    2020-01-01T04:53:33\n",
            "20    2020-01-01T04:55:59\n",
            "Name: timestamp, dtype: object\n",
            "0     10.17\n",
            "1     10.07\n",
            "2      9.90\n",
            "3     10.30\n",
            "4     10.83\n",
            "5      9.80\n",
            "6     10.20\n",
            "7      8.63\n",
            "8      9.77\n",
            "9     10.37\n",
            "10    10.63\n",
            "11     9.47\n",
            "12     8.77\n",
            "13    10.17\n",
            "14     7.97\n",
            "15     9.63\n",
            "16     9.50\n",
            "17     9.55\n",
            "18     9.77\n",
            "19    12.37\n",
            "20    10.53\n",
            "Name: pollution, dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp"
      ],
      "metadata": {
        "id": "D9vaOwIkBFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, count\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('StructuredWebLogExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "  \n",
        "sl = split( lines.value, ' ')\n",
        "table = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string'))\\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string'))\\\n",
        "      .withColumn('location', sl.getItem(2).cast('string'))\\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float'))\\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float'))\\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float'))\\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float'))\\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "try:\n",
        "\n",
        "  results = table \\\n",
        "            .groupBy('location') \\\n",
        "            .count()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "'''\n",
        "x = results.select('timestamp').toList()\n",
        "y = results.select('P2').toList()\n",
        "plt.bar(x,y)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "fuA_-gI1-lbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}