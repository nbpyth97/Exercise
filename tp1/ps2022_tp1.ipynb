{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2022-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbpyth97/Exercise/blob/master/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "30th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi",
        "outputId": "dcd954ad-08eb-45d1-f343-a420b65978aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "outputId": "e0402a37-35d1-4ec0-9970-57811d35d177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 31 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 14.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form",
        "outputId": "ce9de116-3c89-4f45-c4f0-d3af424c2c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting /tmp/kraft-combined-logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1",
        "outputId": "d53c7640-e16d-448e-cb0c-e9fc22f37b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(5, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "#top5 = lines.groupBy(window(lines.timestamp, '24 seconds', '24 seconds'), 'location') \\\n",
        "#       .count()\n",
        "\n",
        "query = lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode('append') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "query.awaitTermination(30)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5fc10bf840c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumpBatchDF\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"Stop this streaming query.\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "\n",
        "sl = split(lines.value, ' ')\n",
        "\n",
        "results = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string')) \\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string')) \\\n",
        "      .withColumn('location', sl.getItem(2).cast('string')) \\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float')) \\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float')) \\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float')) \\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float')) \\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "\n",
        "try:\n",
        "  results = results \\\n",
        "            .withWatermark('timestamp', '1 seconds')\\\n",
        "            .groupBy('location', window( results.timestamp, '10 seconds', '10 seconds')) \\\n",
        "            .count().alias('count')\\\n",
        "\n",
        "  query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(20)\n",
        "  query.stop()\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  "
      ],
      "metadata": {
        "id": "3mvSOvGtApDp",
        "outputId": "1b1b46c4-e552-45b7-fa2b-cf72da1b4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------+-----+\n",
            "|location|window|count|\n",
            "+--------+------+-----+\n",
            "+--------+------+-----+\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|14857   |{2020-01-01 17:05:20, 2020-01-01 17:05:30}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 196, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 193, in call\n",
            "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
            "  File \"<ipython-input-29-54338f2daaeb>\", line 14, in <lambda>\n",
            "    query = results     .writeStream     .outputMode('complete')     .foreachBatch(lambda df, epoch: df.show(10, False))     .start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 502, in show\n",
            "    print(self._jdf.showString(n, int_truncate, vertical))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o2228.showString.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:929)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext,1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "  line = lines.window(1,1)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:('{}{}'.format(x.split(' ')[2],x.split(' ')[-1][8:10]),x.split(' ')[-3]))\\\n",
        "  .map(lambda x:((x[0]),x[1]))\\\n",
        "  .reduceByKey(lambda a,b: max(a+b))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj",
        "outputId": "99584f4d-9ddc-4de6-c401-15793f5795d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o7128.awaitTerminationOrTimeout.\n",
            ": org.apache.spark.SparkException: An exception was raised by Python:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/util.py\", line 68, in call\n",
            "    r = self.func(t, *rdds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
            "    taken = rdd.take(num + 1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1568, in take\n",
            "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/context.py\", line 1227, in runJob\n",
            "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (314b75525b3e executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-48-7a2e24b44f44>\", line 15, in <lambda>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 163, in max\n",
            "    return _invoke_function_over_column(\"max\", col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 66, in _invoke_function_over_column\n",
            "    return _invoke_function(name, _to_java_column(col))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 43, in _to_java_column\n",
            "    jcol = _create_column_from_name(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 36, in _create_column_from_name\n",
            "    return sc._jvm.functions.col(name)\n",
            "AttributeError: 'NoneType' object has no attribute '_jvm'\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
            "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy22.call(Unknown Source)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-48-7a2e24b44f44>\", line 15, in <lambda>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 163, in max\n",
            "    return _invoke_function_over_column(\"max\", col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 66, in _invoke_function_over_column\n",
            "    return _invoke_function(name, _to_java_column(col))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 43, in _to_java_column\n",
            "    jcol = _create_column_from_name(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 36, in _create_column_from_name\n",
            "    return sc._jvm.functions.col(name)\n",
            "AttributeError: 'NoneType' object has no attribute '_jvm'\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\t... 3 more\n",
            "\n",
            "\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average air quality, for each location. Updated hourly;"
      ],
      "metadata": {
        "id": "niYfjlYZ034R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1]))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rZnFTf83TtQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8d2ff1-b997-4f10-c876-6d5439ab80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:47\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:48\n",
            "-------------------------------------------\n",
            "('13691 00', 28.824400000000004)\n",
            "('14857 00', 178.77695652173915)\n",
            "('14858 01', 229.77)\n",
            "('10161 00', 117.895)\n",
            "('14858 00', 263.3808695652174)\n",
            "('2332 00', 549.0566666666666)\n",
            "('14857 01', 129.47)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:49\n",
            "-------------------------------------------\n",
            "('14858 01', 212.37227272727276)\n",
            "('2332 01', 412.6426086956522)\n",
            "('10161 01', 101.8625)\n",
            "('13691 01', 23.940833333333334)\n",
            "('14857 01', 110.07318181818181)\n",
            "('10161 02', 76.43)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:50\n",
            "-------------------------------------------\n",
            "('13691 02', 21.760833333333338)\n",
            "('13691 03', 12.6)\n",
            "('2332 02', 314.77149999999995)\n",
            "('14857 02', 73.19250000000001)\n",
            "('14858 02', 157.10095238095238)\n",
            "('10161 02', 75.21333333333332)\n",
            "('19563 02', 46.72)\n",
            "('14857 03', 60.73)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:51\n",
            "-------------------------------------------\n",
            "('13691 03', 10.992727272727274)\n",
            "('10161 03', 73.2675)\n",
            "('14857 03', 60.79549999999999)\n",
            "('14858 03', 133.3675)\n",
            "('2332 03', 319.23500000000007)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:52\n",
            "-------------------------------------------\n",
            "('2332 04', 256.52521739130435)\n",
            "('14857 04', 46.32909090909091)\n",
            "('14858 04', 121.8)\n",
            "('13691 04', 9.926086956521738)\n",
            "('10161 04', 60.775909090909096)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you show any daily and/or weekly patterns to air quality?"
      ],
      "metadata": {
        "id": "kFNEqiRH-o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#location | timestamp | P1\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "def convert_to_df(rdd):\n",
        "  try:\n",
        "    df = spark.createDataFrame(rdd)\n",
        "    df_pandas = df.toPandas()\n",
        "    df_pandas.columns = ['location', 'timestamp', 'pollution']\n",
        "    x = df_pandas['timestamp']\n",
        "    y = df_pandas['pollution']\n",
        "    plt.bar(x,y)\n",
        "    plt.plot()\n",
        "    plt.xlabel(\"Number of Days\")\n",
        "    plt.ylabel(\"Pollution\")\n",
        "    plt.title(\"Pollution in location 13691\")\n",
        "    plt.legend()\n",
        "\n",
        "    for i in range\n",
        "\n",
        "  except Exception as err:\n",
        "    print(err) \n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "  line = lines.window(1,24)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:(x.split(' ')[2],x.split(' ')[-1][8:10],float(x.split(' ')[-3])))\\\n",
        "  .filter(lambda x : x[0] == '13691')\n",
        "\n",
        "  results.pprint()\n",
        "  results.foreachRDD(convert_to_df)\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(72)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A3AtenvGOGd7",
        "outputId": "2532f8c7-9cc2-48d4-e012-ce4da220c406"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:39:57\n",
            "-------------------------------------------\n",
            "('13691', '01', 40.73)\n",
            "('13691', '01', 44.43)\n",
            "('13691', '01', 42.53)\n",
            "('13691', '01', 43.9)\n",
            "('13691', '01', 42.57)\n",
            "('13691', '01', 40.9)\n",
            "('13691', '01', 42.97)\n",
            "('13691', '01', 39.1)\n",
            "('13691', '01', 35.63)\n",
            "('13691', '01', 37.1)\n",
            "...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:40:21\n",
            "-------------------------------------------\n",
            "('13691', '02', 16.87)\n",
            "('13691', '02', 16.6)\n",
            "('13691', '02', 15.53)\n",
            "('13691', '02', 16.13)\n",
            "('13691', '02', 18.2)\n",
            "('13691', '02', 17.57)\n",
            "('13691', '02', 14.3)\n",
            "('13691', '02', 15.47)\n",
            "('13691', '02', 15.7)\n",
            "('13691', '02', 15.3)\n",
            "...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-29 19:40:45\n",
            "-------------------------------------------\n",
            "('13691', '03', 14.9)\n",
            "('13691', '03', 18.27)\n",
            "('13691', '03', 16.53)\n",
            "('13691', '03', 16.6)\n",
            "('13691', '03', 14.87)\n",
            "('13691', '03', 15.73)\n",
            "('13691', '03', 14.6)\n",
            "('13691', '03', 16.2)\n",
            "('13691', '03', 15.57)\n",
            "('13691', '03', 15.83)\n",
            "...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp"
      ],
      "metadata": {
        "id": "D9vaOwIkBFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, count\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('StructuredWebLogExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "  \n",
        "sl = split( lines.value, ' ')\n",
        "table = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string'))\\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string'))\\\n",
        "      .withColumn('location', sl.getItem(2).cast('string'))\\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float'))\\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float'))\\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float'))\\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float'))\\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "try:\n",
        "\n",
        "  results = table \\\n",
        "            .groupBy('location') \\\n",
        "            .count()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "'''\n",
        "x = results.select('timestamp').toList()\n",
        "y = results.select('P2').toList()\n",
        "plt.bar(x,y)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "fuA_-gI1-lbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}