{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2022-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbpyth97/Exercise/blob/master/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "30th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi",
        "outputId": "c737bd47-f81c-4109-9d61-5cdd7e5b043e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "outputId": "0c1da2a4-e2d5-4dd4-bf60-b66986f69d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form",
        "outputId": "ce9de116-3c89-4f45-c4f0-d3af424c2c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting /tmp/kraft-combined-logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1",
        "outputId": "6bcc72f4-cdbb-4e5a-d8f9-0c1852a76ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(5, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "top5 = lines.groupBy(window(lines.timestamp, '24 seconds', '24 seconds'), 'location') \\\n",
        "       .count()\n",
        "\n",
        "query = top5 \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "query.awaitTermination(30)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-01-31 13:23:36, 2020-01-31 13:24:00}|19563   |1    |\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-02-22 13:50:00, 2020-02-22 13:50:24}|19563   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+--------+-----+\n",
            "|window                                    |location|count|\n",
            "+------------------------------------------+--------+-----+\n",
            "|{2020-03-13 22:14:00, 2020-03-13 22:14:24}|27446   |1    |\n",
            "|{2020-01-23 23:45:12, 2020-01-23 23:45:36}|14857   |1    |\n",
            "|{2020-02-06 12:20:00, 2020-02-06 12:20:24}|25861   |1    |\n",
            "|{2020-03-16 06:36:24, 2020-03-16 06:36:48}|2332    |1    |\n",
            "|{2020-02-01 11:14:48, 2020-02-01 11:15:12}|13691   |1    |\n",
            "+------------------------------------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 196, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 193, in call\n",
            "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
            "  File \"<ipython-input-19-4a3932ad75fa>\", line 6, in dumpBatchDF\n",
            "    df.show(5, False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 502, in show\n",
            "    print(self._jdf.showString(n, int_truncate, vertical))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o1749.showString.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:929)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy25.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO",
        "outputId": "e4e0bcb0-f390-47ec-a5b1-8ffca7ee8160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "\n",
        "sl = split(lines.value, ' ')\n",
        "\n",
        "results = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string')) \\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string')) \\\n",
        "      .withColumn('location', sl.getItem(2).cast('string')) \\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float')) \\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float')) \\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float')) \\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float')) \\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "\n",
        "try:\n",
        "  results = results \\\n",
        "            .withWatermark('timestamp', '1 seconds')\\\n",
        "            .groupBy('location', window( results.timestamp, '10 seconds', '10 seconds')) \\\n",
        "            .count().alias('count')\\\n",
        "\n",
        "  query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(20)\n",
        "  query.stop()\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  "
      ],
      "metadata": {
        "id": "3mvSOvGtApDp",
        "outputId": "1b1b46c4-e552-45b7-fa2b-cf72da1b4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "|10161   |{2020-01-10 15:15:40, 2020-01-10 15:15:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------+-----+\n",
            "|location|window|count|\n",
            "+--------+------+-----+\n",
            "+--------+------+-----+\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "|13691   |{2020-01-10 01:16:30, 2020-01-10 01:16:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "|14858   |{2020-01-01 02:06:40, 2020-01-01 02:06:50}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|2332    |{2020-01-01 02:30:30, 2020-01-01 02:30:40}|1    |\n",
            "|10161   |{2020-01-01 05:15:00, 2020-01-01 05:15:10}|1    |\n",
            "|19563   |{2020-01-01 08:57:20, 2020-01-01 08:57:30}|1    |\n",
            "|14858   |{2020-01-01 05:47:10, 2020-01-01 05:47:20}|1    |\n",
            "|14857   |{2020-01-01 17:05:20, 2020-01-01 17:05:30}|1    |\n",
            "|13691   |{2020-01-01 03:58:50, 2020-01-01 03:59:00}|1    |\n",
            "|2332    |{2020-01-01 02:00:50, 2020-01-01 02:01:00}|1    |\n",
            "|19563   |{2020-01-01 07:55:00, 2020-01-01 07:55:10}|1    |\n",
            "|13691   |{2020-01-01 00:03:40, 2020-01-01 00:03:50}|1    |\n",
            "|10161   |{2020-01-01 12:05:10, 2020-01-01 12:05:20}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------+------------------------------------------+-----+\n",
            "|location|window                                    |count|\n",
            "+--------+------------------------------------------+-----+\n",
            "|14857   |{2020-01-15 17:14:35, 2020-01-15 17:14:45}|1    |\n",
            "|14858   |{2020-01-14 02:37:40, 2020-01-14 02:37:50}|1    |\n",
            "|13691   |{2020-01-11 08:55:35, 2020-01-11 08:55:45}|1    |\n",
            "|13691   |{2020-01-03 17:45:00, 2020-01-03 17:45:10}|1    |\n",
            "|19563   |{2020-01-12 23:45:35, 2020-01-12 23:45:45}|1    |\n",
            "|19563   |{2020-01-16 07:44:30, 2020-01-16 07:44:40}|1    |\n",
            "|14857   |{2020-01-06 17:15:30, 2020-01-06 17:15:40}|1    |\n",
            "|2332    |{2020-01-03 05:32:00, 2020-01-03 05:32:10}|1    |\n",
            "|19563   |{2020-01-02 17:31:30, 2020-01-02 17:31:40}|1    |\n",
            "|13691   |{2020-01-03 22:56:30, 2020-01-03 22:56:40}|1    |\n",
            "+--------+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 196, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 193, in call\n",
            "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
            "  File \"<ipython-input-29-54338f2daaeb>\", line 14, in <lambda>\n",
            "    query = results     .writeStream     .outputMode('complete')     .foreachBatch(lambda df, epoch: df.show(10, False))     .start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 502, in show\n",
            "    print(self._jdf.showString(n, int_truncate, vertical))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o2228.showString.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:929)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext,1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "  line = lines.window(24,24)\n",
        "\n",
        "  results=line.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][8:10]),x.split(' ')[-3]))\\\n",
        "  .reduceByKey(lambda a,b: max(a,b))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(75)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj",
        "outputId": "77b7c98b-900f-4e8f-e37b-c9e497e166cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o1957.awaitTerminationOrTimeout.\n",
            ": org.apache.spark.SparkException: An exception was raised by Python:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/util.py\", line 68, in call\n",
            "    r = self.func(t, *rdds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
            "    taken = rdd.take(num + 1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1568, in take\n",
            "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/context.py\", line 1227, in runJob\n",
            "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\", line 111, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (a3abc00cd3e8 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-21-dde93380c196>\", line 15, in <lambda>\n",
            "TypeError: max() takes 1 positional argument but 2 were given\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
            "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy20.call(Unknown Source)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
            "    process()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
            "    out_iter = func(split_index, iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
            "    return func(split, prev_func(split, iterator))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 417, in func\n",
            "    return f(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
            "    merger.mergeValues(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n",
            "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/util.py\", line 74, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<ipython-input-21-dde93380c196>\", line 15, in <lambda>\n",
            "TypeError: max() takes 1 positional argument but 2 were given\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
            "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
            "\t... 3 more\n",
            "\n",
            "\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
            "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
            "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
            "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average air quality, for each location. Updated hourly;"
      ],
      "metadata": {
        "id": "niYfjlYZ034R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1]))\n",
        "\n",
        "  results.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rZnFTf83TtQA",
        "outputId": "ab8d2ff1-b997-4f10-c876-6d5439ab80e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:47\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:48\n",
            "-------------------------------------------\n",
            "('13691 00', 28.824400000000004)\n",
            "('14857 00', 178.77695652173915)\n",
            "('14858 01', 229.77)\n",
            "('10161 00', 117.895)\n",
            "('14858 00', 263.3808695652174)\n",
            "('2332 00', 549.0566666666666)\n",
            "('14857 01', 129.47)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:49\n",
            "-------------------------------------------\n",
            "('14858 01', 212.37227272727276)\n",
            "('2332 01', 412.6426086956522)\n",
            "('10161 01', 101.8625)\n",
            "('13691 01', 23.940833333333334)\n",
            "('14857 01', 110.07318181818181)\n",
            "('10161 02', 76.43)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:50\n",
            "-------------------------------------------\n",
            "('13691 02', 21.760833333333338)\n",
            "('13691 03', 12.6)\n",
            "('2332 02', 314.77149999999995)\n",
            "('14857 02', 73.19250000000001)\n",
            "('14858 02', 157.10095238095238)\n",
            "('10161 02', 75.21333333333332)\n",
            "('19563 02', 46.72)\n",
            "('14857 03', 60.73)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:51\n",
            "-------------------------------------------\n",
            "('13691 03', 10.992727272727274)\n",
            "('10161 03', 73.2675)\n",
            "('14857 03', 60.79549999999999)\n",
            "('14858 03', 133.3675)\n",
            "('2332 03', 319.23500000000007)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:04:52\n",
            "-------------------------------------------\n",
            "('2332 04', 256.52521739130435)\n",
            "('14857 04', 46.32909090909091)\n",
            "('14858 04', 121.8)\n",
            "('13691 04', 9.926086956521738)\n",
            "('10161 04', 60.775909090909096)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you show any daily and/or weekly patterns to air quality?"
      ],
      "metadata": {
        "id": "kFNEqiRH-o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data_list=[]\n",
        "\n",
        "def convert_to_df(rdd):\n",
        "    df = spark.createDataFrame(rdd)\n",
        "    df.show(10)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:(x.split(' ')[2],x.split(' ')[-1],float(x.split(' ')[-3])))\\\n",
        "  .filter(lambda x : x[0]) #location | timestamp | P1\n",
        "\n",
        "  results.pprint()\n",
        "  results.foreachRDD(convert_to_df)\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "A3AtenvGOGd7",
        "outputId": "d67a76b5-17cd-4eb2-a681-3e58bda4d811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:52:27\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T00:01:21', 19.83)\n",
            "('10161', '2020-01-01T00:01:55', 109.27)\n",
            "('14858', '2020-01-01T00:02:15', 348.83)\n",
            "('14857', '2020-01-01T00:02:15', 408.73)\n",
            "('13691', '2020-01-01T00:03:47', 19.07)\n",
            "('10161', '2020-01-01T00:04:28', 111.63)\n",
            "('14857', '2020-01-01T00:04:47', 204.17)\n",
            "('14858', '2020-01-01T00:04:47', 314.53)\n",
            "('13691', '2020-01-01T00:06:13', 15.73)\n",
            "('10161', '2020-01-01T00:06:54', 98.8)\n",
            "...\n",
            "\n",
            "+-----+-------------------+------+\n",
            "|   _1|                 _2|    _3|\n",
            "+-----+-------------------+------+\n",
            "|13691|2020-01-01T00:01:21| 19.83|\n",
            "|10161|2020-01-01T00:01:55|109.27|\n",
            "|14858|2020-01-01T00:02:15|348.83|\n",
            "|14857|2020-01-01T00:02:15|408.73|\n",
            "|13691|2020-01-01T00:03:47| 19.07|\n",
            "|10161|2020-01-01T00:04:28|111.63|\n",
            "|14857|2020-01-01T00:04:47|204.17|\n",
            "|14858|2020-01-01T00:04:47|314.53|\n",
            "|13691|2020-01-01T00:06:13| 15.73|\n",
            "|10161|2020-01-01T00:06:54|  98.8|\n",
            "+-----+-------------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:52:28\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T00:45:14', 14.97)\n",
            "('14857', '2020-01-01T00:45:15', 137.07)\n",
            "('14858', '2020-01-01T00:45:15', 217.93)\n",
            "('2332', '2020-01-01T00:46:26', 540.87)\n",
            "('10161', '2020-01-01T00:46:53', 114.67)\n",
            "('13691', '2020-01-01T00:47:41', 14.57)\n",
            "('14857', '2020-01-01T00:47:43', 132.33)\n",
            "('14858', '2020-01-01T00:47:43', 215.0)\n",
            "('2332', '2020-01-01T00:48:53', 547.7)\n",
            "('10161', '2020-01-01T00:49:21', 110.3)\n",
            "...\n",
            "\n",
            "+-----+-------------------+------+\n",
            "|   _1|                 _2|    _3|\n",
            "+-----+-------------------+------+\n",
            "|13691|2020-01-01T00:45:14| 14.97|\n",
            "|14857|2020-01-01T00:45:15|137.07|\n",
            "|14858|2020-01-01T00:45:15|217.93|\n",
            "| 2332|2020-01-01T00:46:26|540.87|\n",
            "|10161|2020-01-01T00:46:53|114.67|\n",
            "|13691|2020-01-01T00:47:41| 14.57|\n",
            "|14857|2020-01-01T00:47:43|132.33|\n",
            "|14858|2020-01-01T00:47:43| 215.0|\n",
            "| 2332|2020-01-01T00:48:53| 547.7|\n",
            "|10161|2020-01-01T00:49:21| 110.3|\n",
            "+-----+-------------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:52:29\n",
            "-------------------------------------------\n",
            "('10161', '2020-01-01T01:45:51', 83.27)\n",
            "('2332', '2020-01-01T01:46:03', 370.57)\n",
            "('14858', '2020-01-01T01:46:21', 225.87)\n",
            "('13691', '2020-01-01T01:46:35', 13.5)\n",
            "('14857', '2020-01-01T01:46:40', 96.67)\n",
            "('10161', '2020-01-01T01:48:17', 83.03)\n",
            "('2332', '2020-01-01T01:48:29', 405.9)\n",
            "('14858', '2020-01-01T01:48:49', 185.43)\n",
            "('13691', '2020-01-01T01:49:01', 11.63)\n",
            "('14857', '2020-01-01T01:49:08', 74.77)\n",
            "...\n",
            "\n",
            "+-----+-------------------+------+\n",
            "|   _1|                 _2|    _3|\n",
            "+-----+-------------------+------+\n",
            "|10161|2020-01-01T01:45:51| 83.27|\n",
            "| 2332|2020-01-01T01:46:03|370.57|\n",
            "|14858|2020-01-01T01:46:21|225.87|\n",
            "|13691|2020-01-01T01:46:35|  13.5|\n",
            "|14857|2020-01-01T01:46:40| 96.67|\n",
            "|10161|2020-01-01T01:48:17| 83.03|\n",
            "| 2332|2020-01-01T01:48:29| 405.9|\n",
            "|14858|2020-01-01T01:48:49|185.43|\n",
            "|13691|2020-01-01T01:49:01| 11.63|\n",
            "|14857|2020-01-01T01:49:08| 74.77|\n",
            "+-----+-------------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:52:30\n",
            "-------------------------------------------\n",
            "('13691', '2020-01-01T02:45:08', 11.17)\n",
            "('2332', '2020-01-01T02:45:55', 182.47)\n",
            "('14857', '2020-01-01T02:46:13', 64.67)\n",
            "('14858', '2020-01-01T02:46:19', 142.0)\n",
            "('10161', '2020-01-01T02:46:44', 75.8)\n",
            "('13691', '2020-01-01T02:47:34', 13.03)\n",
            "('2332', '2020-01-01T02:48:27', 197.57)\n",
            "('14858', '2020-01-01T02:48:47', 143.5)\n",
            "('10161', '2020-01-01T02:49:10', 74.93)\n",
            "('13691', '2020-01-01T02:50:16', 15.23)\n",
            "...\n",
            "\n",
            "+-----+-------------------+------+\n",
            "|   _1|                 _2|    _3|\n",
            "+-----+-------------------+------+\n",
            "|13691|2020-01-01T02:45:08| 11.17|\n",
            "| 2332|2020-01-01T02:45:55|182.47|\n",
            "|14857|2020-01-01T02:46:13| 64.67|\n",
            "|14858|2020-01-01T02:46:19| 142.0|\n",
            "|10161|2020-01-01T02:46:44|  75.8|\n",
            "|13691|2020-01-01T02:47:34| 13.03|\n",
            "| 2332|2020-01-01T02:48:27|197.57|\n",
            "|14858|2020-01-01T02:48:47| 143.5|\n",
            "|10161|2020-01-01T02:49:10| 74.93|\n",
            "|13691|2020-01-01T02:50:16| 15.23|\n",
            "+-----+-------------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-04-28 22:52:31\n",
            "-------------------------------------------\n",
            "('2332', '2020-01-01T03:45:04', 209.77)\n",
            "('10161', '2020-01-01T03:45:36', 74.73)\n",
            "('14858', '2020-01-01T03:46:03', 132.77)\n",
            "('2332', '2020-01-01T03:47:54', 175.4)\n",
            "('10161', '2020-01-01T03:48:06', 64.27)\n",
            "('14858', '2020-01-01T03:49:02', 129.5)\n",
            "('14857', '2020-01-01T03:50:46', 95.35)\n",
            "('2332', '2020-01-01T03:51:01', 207.33)\n",
            "('13691', '2020-01-01T03:51:06', 9.73)\n",
            "('10161', '2020-01-01T03:51:09', 67.63)\n",
            "...\n",
            "\n",
            "+-----+-------------------+------+\n",
            "|   _1|                 _2|    _3|\n",
            "+-----+-------------------+------+\n",
            "| 2332|2020-01-01T03:45:04|209.77|\n",
            "|10161|2020-01-01T03:45:36| 74.73|\n",
            "|14858|2020-01-01T03:46:03|132.77|\n",
            "| 2332|2020-01-01T03:47:54| 175.4|\n",
            "|10161|2020-01-01T03:48:06| 64.27|\n",
            "|14858|2020-01-01T03:49:02| 129.5|\n",
            "|14857|2020-01-01T03:50:46| 95.35|\n",
            "| 2332|2020-01-01T03:51:01|207.33|\n",
            "|13691|2020-01-01T03:51:06|  9.73|\n",
            "|10161|2020-01-01T03:51:09| 67.63|\n",
            "+-----+-------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp"
      ],
      "metadata": {
        "id": "D9vaOwIkBFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, count\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('StructuredWebLogExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark.readStream.format('socket') \\\n",
        "    .option('host', 'localhost') \\\n",
        "    .option('port', 7777) \\\n",
        "    .load()\n",
        "  \n",
        "sl = split( lines.value, ' ')\n",
        "table = lines \\\n",
        "      .withColumn('sensor_id', sl.getItem(0).cast('string'))\\\n",
        "      .withColumn('sensor_type', sl.getItem(1).cast('string'))\\\n",
        "      .withColumn('location', sl.getItem(2).cast('string'))\\\n",
        "      .withColumn('latitude', sl.getItem(3).cast('float'))\\\n",
        "      .withColumn('longitude', sl.getItem(4).cast('float'))\\\n",
        "      .withColumn('P1', sl.getItem(5).cast('float'))\\\n",
        "      .withColumn('P2', sl.getItem(6).cast('float'))\\\n",
        "      .withColumn('timestamp', sl.getItem(7).cast('timestamp'))\\\n",
        "      .drop('value')\n",
        "try:\n",
        "\n",
        "  results = table \\\n",
        "            .groupBy('location') \\\n",
        "            .count()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "'''\n",
        "x = results.select('timestamp').toList()\n",
        "y = results.select('P2').toList()\n",
        "plt.bar(x,y)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "fuA_-gI1-lbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  results=lines.filter(lambda x : len(x)>0)\\\n",
        "  .map(lambda x:((x.split(' ')[2]+' '+x.split(' ')[-1][11:13]),(float(x.split(' ')[-3]),1)))\\\n",
        "  .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
        "  .map(lambda x : (x[0],x[1][0]/x[1][1])).dumpBatchDF()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "KmKqnpsQSMba",
        "outputId": "8f39bdc4-d43d-4235-9574-954f53da72ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'TransformedDStream' object has no attribute 'dumpBatchDF'\n"
          ]
        }
      ]
    }
  ]
}